<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Age, Gender & Emotion Scanner</title>

  <!-- face-api.js -->
  <script src="https://unpkg.com/face-api.js@0.22.2/dist/face-api.min.js"></script>

  <style>
    body {
      background: #111;
      color: #eee;
      text-align: center;
      font-family: sans-serif;
      padding: 20px;
    }
    video, canvas {
      width: 100%;
      max-width: 480px;
      border-radius: 12px;
    }
    button {
      padding: 10px 20px;
      margin: 10px;
      border: none;
      border-radius: 6px;
      font-size: 16px;
      cursor: pointer;
    }
    #startBtn { background: #28a745; color: white; }
    #stopBtn { background: #dc3545; color: white; }
  </style>
</head>

<body>
  <h1>Age, Gender & Emotion Scanner</h1>
  <p id="status">Loading models...</p>

  <div>
    <video id="video" autoplay muted playsinline></video>
    <canvas id="overlay"></canvas>
  </div>

  <button id="startBtn">Start</button>
  <button id="stopBtn">Stop</button>

  <script>
    const video = document.getElementById("video");
    const overlay = document.getElementById("overlay");
    const statusEl = document.getElementById("status");

    let stream = null;
    let interval = null;

    // CDN-hosted model folder:
    const MODEL_URL = "https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/weights/";

    async function loadModels() {
      statusEl.textContent = "Loading models...";

      await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
      await faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL);
      await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
      await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);

      statusEl.textContent = "Models loaded.";
    }

    async function startCamera() {
      try {
        stream = await navigator.mediaDevices.getUserMedia({ video: true });
        video.srcObject = stream;
        await video.play();
        statusEl.textContent = "Camera started.";

        startDetection();
      } catch (e) {
        statusEl.textContent = "Camera error: " + e;
      }
    }

    function stopCamera() {
      clearInterval(interval);
      if (stream) stream.getTracks().forEach(t => t.stop());
      statusEl.textContent = "Camera stopped.";
    }

    function startDetection() {
      interval = setInterval(async () => {
        if (video.readyState !== 4) return;

        const detections = await faceapi
          .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
          .withAgeAndGender()
          .withFaceExpressions()
          .withFaceLandmarks();

        draw(detections);
      }, 200);
    }

    function draw(results) {
      const ctx = overlay.getContext("2d");
      overlay.width = video.videoWidth;
      overlay.height = video.videoHeight;

      ctx.clearRect(0, 0, overlay.width, overlay.height);

      results.forEach(det => {
        const box = det.detection.box;

        // Box
        ctx.strokeStyle = "#00FF00";
        ctx.lineWidth = 2;
        ctx.strokeRect(box.x, box.y, box.width, box.height);

        // Find top emotion
        let topExp = "neutral";
        let topScore = 0;
        for (const [name, score] of Object.entries(det.expressions)) {
          if (score > topScore) {
            topScore = score;
            topExp = name;
          }
        }

        const label =
          `Age: ${Math.round(det.age)} | ` +
          `${det.gender} | ` +
          `${topExp}`;

        ctx.fillStyle = "rgba(0,0,0,0.5)";
        ctx.fillRect(box.x, box.y - 20, ctx.measureText(label).width + 10, 20);

        ctx.fillStyle = "white";
        ctx.fillText(label, box.x + 5, box.y - 5);
      });
    }

    document.getElementById("startBtn").onclick = startCamera;
    document.getElementById("stopBtn").onclick = stopCamera;

    loadModels();
  </script>
</body>
</html>
